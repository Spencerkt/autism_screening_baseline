{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting some baselines from various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display metrics using show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(meanVar, conf_mat, SMOTE=False):\n",
    "    if SMOTE:\n",
    "        print('Preprocessed with SMOTE')\n",
    "    else:\n",
    "        print('Original data')\n",
    "    print('Mean Accuracy: \\t\\t', meanVar[0])\n",
    "    print('Variance from Cross Validation: \\t', meanVar[1])\n",
    "    print('Confusion Matrix: \\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            f   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            m   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            m   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            f   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            f   \n",
       "\n",
       "        ethnicity jundice autism        country app result  age_category  \\\n",
       "0  White-European      no     no  United States  no      6   18 and more   \n",
       "1          Latino      no    yes         Brazil  no      5   18 and more   \n",
       "2          Latino     yes    yes          Spain  no      8   18 and more   \n",
       "3  White-European      no    yes  United States  no      6   18 and more   \n",
       "4               ?      no     no          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Autism_Data.csv')\n",
    "df.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', \n",
    "              'A7', 'A8', 'A9', 'A10', 'age', 'gender',\n",
    "             'ethnicity', 'jundice', 'autism', 'country', \n",
    "             'app', 'result', 'age_category', 'relation',\n",
    "             'app_prediction']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert yes/no colums to 1/0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            0   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            1   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            1   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            0   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            0   \n",
       "\n",
       "        ethnicity jundice  autism        country app result  age_category  \\\n",
       "0  White-European       0       0  United States  no      6   18 and more   \n",
       "1          Latino       0       1         Brazil  no      5   18 and more   \n",
       "2          Latino       1       1          Spain  no      8   18 and more   \n",
       "3  White-European       0       1  United States  no      6   18 and more   \n",
       "4               ?       0       0          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_row = pd.Series(np.where(df['gender'] == 'm', 1, 0))\n",
    "jund_row = pd.Series(np.where(df['jundice'] == 'yes', 1, 0))\n",
    "autism_row = pd.Series(np.where(df['autism'] == 'yes', 1, 0))\n",
    "\n",
    "df['gender'] = gender_row\n",
    "df['jundice'] = jund_row\n",
    "df['autism'] = autism_row\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish X, y datasets.  \n",
    "### Set global paramaters for models found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'gender', 'jundice']]\n",
    "y = df['autism']\n",
    "\n",
    "test_size = 0.3\n",
    "random_state = 1\n",
    "alpha = 1e-5\n",
    "cv = 3\n",
    "\n",
    "# solver = {sgd, adam, lbfgs}\n",
    "solver = 'lbfgs'\n",
    "\n",
    "# hidden_layer_sizes = tuple, ith element is number of neurons in ith layer.\n",
    "# best results with {(30, 20, 10), (20, 30, 20, 10)}\n",
    "hidden_layer_sizes = (30, 20, 10)\n",
    "\n",
    "orig_text = 'Original data'\n",
    "smote_text = 'Preprocessed with SMOTE'\n",
    "\n",
    "# Dicts to keep track of scores\n",
    "orig_dict = {}\n",
    "smote_dict = {}\n",
    "\n",
    "# For training on original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "# For training on data after SMOTE \n",
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "sX_train, sX_test, sy_train, sy_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Mean Accuracy: \t\t 0.7869887488531556\n",
      "Variance from Cross Validation: \t 0.0002768878717148167\n",
      "Confusion Matrix: \n",
      " [[175  19]\n",
      " [ 13   5]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlp_cross_val = cross_val_score(mlp, X, y, cv=cv)\n",
    "mlp_conf_mat = confusion_matrix(y_test, mlp.predict(X_test))\n",
    "\n",
    "orig_dict['MLP'] = (np.mean(mlp_cross_val), np.var(mlp_cross_val))\n",
    "show_results(orig_dict['MLP'], mlp_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (data after SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed with SMOTE\n",
      "Mean Accuracy: \t\t 0.863868962219034\n",
      "Variance from Cross Validation: \t 0.0013991708027609982\n",
      "Confusion Matrix: \n",
      " [[163  36]\n",
      " [  5 164]]\n"
     ]
    }
   ],
   "source": [
    "mpl_smote = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "mpl_smote.fit(sX_train, sy_train)\n",
    "\n",
    "mlp_cross_val = cross_val_score(mpl_smote, X_smote, y_smote, cv=cv)\n",
    "mlp_conf_mat = confusion_matrix(sy_test, mpl_smote.predict(sX_test))\n",
    "\n",
    "smote_dict['MLP'] = (np.mean(mlp_cross_val), np.var(mlp_cross_val))\n",
    "show_results(smote_dict['MLP'], mlp_conf_mat, SMOTE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Mean Accuracy: \t\t 0.8707446037954513\n",
      "Variance from Cross Validation: \t 2.2061257412136467e-06\n",
      "Confusion Matrix: \n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "svm_cross_val = cross_val_score(clf_svm, X, y, cv=cv)\n",
    "svm_conf_mat = confusion_matrix(y_test, clf_svm.predict(X_test))\n",
    "\n",
    "orig_dict['SVM'] = (np.mean(svm_cross_val), np.var(svm_cross_val))\n",
    "show_results(orig_dict['SVM'], svm_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed with SMOTE\n",
      "Mean Accuracy: \t\t 0.7324964131994262\n",
      "Variance from Cross Validation: \t 0.0006906604737130739\n",
      "Confusion Matrix: \n",
      " [[127  72]\n",
      " [ 39 130]]\n"
     ]
    }
   ],
   "source": [
    "clf_svm_smote = svm.SVC()\n",
    "clf_svm_smote.fit(sX_train, sy_train)\n",
    "\n",
    "svm_cross_val = cross_val_score(clf_svm_smote, X_smote, y_smote, cv=cv)\n",
    "svm_conf_mat = confusion_matrix(sy_test, clf_svm_smote.predict(sX_test))\n",
    "\n",
    "smote_dict['SVM'] = (np.mean(svm_cross_val), np.var(svm_cross_val))\n",
    "show_results(smote_dict['SVM'], svm_conf_mat, SMOTE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Mean Accuracy: \t\t 0.8452001545221884\n",
      "Variance from Cross Validation: \t 5.464715406188476e-05\n",
      "Confusion Matrix: \n",
      " [[186   8]\n",
      " [ 17   1]]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_cross_val = cross_val_score(knn, X, y, cv=cv)\n",
    "knn_conf_mat = confusion_matrix(y_test, knn.predict(X_test))\n",
    "\n",
    "orig_dict['KNN'] = (np.mean(knn_cross_val), np.var(knn_cross_val))\n",
    "show_results(orig_dict['KNN'], knn_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed with SMOTE\n",
      "Mean Accuracy: \t\t 0.7969193368404272\n",
      "Variance from Cross Validation: \t 6.774270429757122e-05\n",
      "Confusion Matrix: \n",
      " [[131  68]\n",
      " [  5 164]]\n"
     ]
    }
   ],
   "source": [
    "knn_smote = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_smote.fit(sX_train, sy_train)\n",
    "\n",
    "knn_cross_val = cross_val_score(knn_smote, X_smote, y_smote, cv=cv)\n",
    "knn_conf_mat = confusion_matrix(sy_test, knn_smote.predict(sX_test))\n",
    "\n",
    "smote_dict['KNN'] = (np.mean(knn_cross_val), np.var(knn_cross_val))\n",
    "show_results(smote_dict['KNN'], knn_conf_mat, SMOTE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Mean Accuracy: \t\t 0.8707446037954513\n",
      "Variance from Cross Validation: \t 2.2061257412136467e-06\n",
      "Confusion Matrix: \n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "gpc = GaussianProcessClassifier()\n",
    "gpc.fit(X_train, y_train)\n",
    "\n",
    "gpc_cross_val = cross_val_score(gpc, X, y, cv=cv)\n",
    "gpc_conf_mat = confusion_matrix(y_test, gpc.predict(X_test))\n",
    "\n",
    "orig_dict['GPC'] = (np.mean(gpc_cross_val), np.var(gpc_cross_val))\n",
    "show_results(orig_dict['GPC'], gpc_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed with SMOTE\n",
      "Mean Accuracy: \t\t 0.7952813645783516\n",
      "Variance from Cross Validation: \t 6.399275470985418e-05\n",
      "Confusion Matrix: \n",
      " [[138  61]\n",
      " [ 26 143]]\n"
     ]
    }
   ],
   "source": [
    "gpc_smote = GaussianProcessClassifier()\n",
    "gpc_smote.fit(sX_train, sy_train)\n",
    "\n",
    "gpc_cross_val = cross_val_score(gpc_smote, X_smote, y_smote, cv=cv)\n",
    "gpc_conf_mat = confusion_matrix(sy_test, gpc_smote.predict(sX_test))\n",
    "\n",
    "smote_dict['GPC'] = (np.mean(gpc_cross_val), np.var(gpc_cross_val))\n",
    "show_results(smote_dict['GPC'], gpc_conf_mat, SMOTE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Mean Accuracy: \t\t 0.8678956009464484\n",
      "Variance from Cross Validation: \t 0.00010985713396685145\n",
      "Confusion Matrix: \n",
      " [[193   1]\n",
      " [ 17   1]]\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "\n",
    "lgr_cross_val = cross_val_score(lgr, X, y, cv=cv)\n",
    "lgr_conf_mat = confusion_matrix(y_test, lgr.predict(X_test))\n",
    "\n",
    "orig_dict['LGR'] = (np.mean(lgr_cross_val), np.var(lgr_cross_val))\n",
    "show_results(orig_dict['LGR'], lgr_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression after SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed with SMOTE\n",
      "Mean Accuracy: \t\t 0.6941415590626495\n",
      "Variance from Cross Validation: \t 0.00018268181985558552\n",
      "Confusion Matrix: \n",
      " [[116  83]\n",
      " [ 42 127]]\n"
     ]
    }
   ],
   "source": [
    "lgr_smote = LogisticRegression()\n",
    "lgr_smote.fit(sX_train, sy_train)\n",
    "\n",
    "lgr_cross_val = cross_val_score(lgr_smote, X_smote, y_smote, cv=cv)\n",
    "lgr_conf_mat = confusion_matrix(sy_test, lgr_smote.predict(sX_test))\n",
    "\n",
    "smote_dict['LGR'] = (np.mean(lgr_cross_val), np.var(lgr_cross_val))\n",
    "show_results(smote_dict['LGR'], lgr_conf_mat, SMOTE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without smote:\n",
      "MLP :   (0.7869887488531556, 0.0002768878717148167)\n",
      "SVM :   (0.8707446037954513, 2.2061257412136467e-06)\n",
      "KNN :   (0.8452001545221884, 5.464715406188476e-05)\n",
      "GPC :   (0.8707446037954513, 2.2061257412136467e-06)\n",
      "LGR :   (0.8678956009464484, 0.00010985713396685145)\n",
      "\n",
      "\n",
      "with smote:\n",
      "MLP :   (0.863868962219034, 0.0013991708027609982)\n",
      "SVM :   (0.7324964131994262, 0.0006906604737130739)\n",
      "KNN :   (0.7969193368404272, 6.774270429757122e-05)\n",
      "GPC :   (0.7952813645783516, 6.399275470985418e-05)\n",
      "LGR :   (0.6941415590626495, 0.00018268181985558552)\n"
     ]
    }
   ],
   "source": [
    "print('without smote:')\n",
    "for key, val in orig_dict.items():\n",
    "    print(key, ':  ', val)\n",
    "\n",
    "print('\\n\\nwith smote:')\n",
    "for key, val in smote_dict.items():\n",
    "    print(key, ':  ', val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
