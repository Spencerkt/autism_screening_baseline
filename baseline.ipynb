{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting some baselines from various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display metrics using show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_results(test_type, accuracy, cross_val, conf_mat):\n",
    "    print()\n",
    "    print(test_type)\n",
    "    print('Accuracy:\\t\\t\\t', accuracy)\n",
    "    print('Cross Validation Results:\\t', cross_val)\n",
    "    print('Cross Validation Mean:\\t\\t', np.mean(cross_val))\n",
    "    print('Confusion Matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            f   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            m   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            m   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            f   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            f   \n",
       "\n",
       "        ethnicity jundice autism        country app result  age_category  \\\n",
       "0  White-European      no     no  United States  no      6   18 and more   \n",
       "1          Latino      no    yes         Brazil  no      5   18 and more   \n",
       "2          Latino     yes    yes          Spain  no      8   18 and more   \n",
       "3  White-European      no    yes  United States  no      6   18 and more   \n",
       "4               ?      no     no          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Autism_Data.csv')\n",
    "df.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', \n",
    "              'A7', 'A8', 'A9', 'A10', 'age', 'gender',\n",
    "             'ethnicity', 'jundice', 'autism', 'country', \n",
    "             'app', 'result', 'age_category', 'relation',\n",
    "             'app_prediction']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert yes/no colums to 1/0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            0   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            1   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            1   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            0   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            0   \n",
       "\n",
       "        ethnicity jundice  autism        country app result  age_category  \\\n",
       "0  White-European       0       0  United States  no      6   18 and more   \n",
       "1          Latino       0       1         Brazil  no      5   18 and more   \n",
       "2          Latino       1       1          Spain  no      8   18 and more   \n",
       "3  White-European       0       1  United States  no      6   18 and more   \n",
       "4               ?       0       0          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_row = pd.Series(np.where(df['gender'] == 'm', 1, 0))\n",
    "jund_row = pd.Series(np.where(df['jundice'] == 'yes', 1, 0))\n",
    "autism_row = pd.Series(np.where(df['autism'] == 'yes', 1, 0))\n",
    "\n",
    "df['gender'] = gender_row\n",
    "df['jundice'] = jund_row\n",
    "df['autism'] = autism_row\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish X, y datasets.  \n",
    "### Set global paramaters for models found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'gender', 'jundice']]\n",
    "y = df['autism']\n",
    "\n",
    "test_size = 0.3\n",
    "random_state = 1\n",
    "alpha = 1e-5\n",
    "cv = 5\n",
    "\n",
    "# solver = {sgd, adam, lbfgs}\n",
    "solver = 'lbfgs'\n",
    "\n",
    "# hidden_layer_sizes = tuple, ith element is number of neurons in ith layer.\n",
    "hidden_layer_sizes = (20, 14)\n",
    "\n",
    "orig_text = 'Original data'\n",
    "smote_text = 'Preprocessed with SMOTE'\n",
    "\n",
    "# Dicts to keep track of scores\n",
    "orig_dict = {}\n",
    "smote_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.8443396226415094\n",
      "Cross Validation Results:\t [0.80985915 0.82269504 0.78014184 0.80714286 0.85      ]\n",
      "Cross Validation Mean:\t\t 0.8139677783010116\n",
      "Confusion Matrix:\n",
      " [[174  20]\n",
      " [ 13   5]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "clf = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "orig_dict['MLP'] = accuracy\n",
    "cross_val = cross_val_score(clf, X, y, cv=cv)\n",
    "conf_mat = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "show_results(orig_text, accuracy, cross_val, conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (data after SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.8443396226415094\n",
      "Cross Validation Results:\t [0.80487805 0.88211382 0.89430894 0.89344262 0.8852459 ]\n",
      "Cross Validation Mean:\t\t 0.8719978675196588\n",
      "Confusion Matrix:\n",
      " [[162  37]\n",
      " [ 12 157]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "clf = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "smote_accuracy = clf.score(X_test, y_test)\n",
    "smote_dict['MLP'] = smote_accuracy\n",
    "cross_val = cross_val_score(clf, X_smote, y_smote, cv=5)\n",
    "conf_mat = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "show_results(smote_text, accuracy, cross_val, conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.9150943396226415\n",
      "Cross Validation Results:\t [0.86864407 0.87179487 0.87179487]\n",
      "Cross Validation Mean:\t\t 0.8707446037954513\n",
      "Confusion Matrix:\n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "svm_accuracy = clf_svm.score(X_test, y_test)\n",
    "orig_dict['SVM'] = svm_accuracy\n",
    "cross_val_svm = cross_val_score(clf_svm, X, y)\n",
    "conf_mat_svm = confusion_matrix(y_test, clf_svm.predict(X_test))\n",
    "\n",
    "show_results(orig_text, svm_accuracy, cross_val_svm, conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.7119565217391305\n",
      "Cross Validation Results:\t [0.86864407 0.87179487 0.87179487]\n",
      "Cross Validation Mean:\t\t 0.8707446037954513\n",
      "Confusion Matrix:\n",
      " [[126  73]\n",
      " [ 33 136]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "clf_svm_smote = svm.SVC()\n",
    "clf_svm_smote.fit(X_train, y_train)\n",
    "\n",
    "svm_accuracy = clf_svm_smote.score(X_test, y_test)\n",
    "smote_dict['SVM'] = svm_accuracy\n",
    "cross_val_svm = cross_val_score(clf_svm_smote, X, y)\n",
    "conf_mat_svm = confusion_matrix(y_test, clf_svm_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, svm_accuracy, cross_val_svm, conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.8820754716981132\n",
      "Cross Validation Results:\t [0.83474576 0.85042735 0.85042735]\n",
      "Cross Validation Mean:\t\t 0.8452001545221884\n",
      "Confusion Matrix:\n",
      " [[186   8]\n",
      " [ 17   1]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_accuracy = knn.score(X_test, y_test)\n",
    "orig_dict['KNN'] = knn_accuracy\n",
    "cross_val_knn = cross_val_score(knn, X, y)\n",
    "conf_mat_knn = confusion_matrix(y_test, knn.predict(X_test))\n",
    "\n",
    "show_results(orig_text, knn_accuracy, cross_val_knn, conf_mat_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.8016304347826086\n",
      "Cross Validation Results:\t [0.75121951 0.80882353 0.84313725]\n",
      "Cross Validation Mean:\t\t 0.8010600988362825\n",
      "Confusion Matrix:\n",
      " [[134  65]\n",
      " [  8 161]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_smote.fit(X_train, y_train)\n",
    "\n",
    "knn_accuracy = knn_smote.score(X_test, y_test)\n",
    "smote_dict['KNN'] = knn_accuracy\n",
    "cross_val_knn = cross_val_score(knn_smote, X_smote, y_smote)\n",
    "conf_mat_knn = confusion_matrix(y_test, knn_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, knn_accuracy, cross_val_knn, conf_mat_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.9150943396226415\n",
      "Cross Validation Results:\t [0.86864407 0.87179487 0.87179487]\n",
      "Cross Validation Mean:\t\t 0.8707446037954513\n",
      "Confusion Matrix:\n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "gpc = GaussianProcessClassifier()\n",
    "gpc.fit(X_train, y_train)\n",
    "\n",
    "gpc_accuracy = gpc.score(X_test, y_test)\n",
    "orig_dict['GPC'] = gpc_accuracy\n",
    "cross_val_gpc = cross_val_score(gpc, X, y)\n",
    "conf_mat_gpc = confusion_matrix(y_test, gpc.predict(X_test))\n",
    "\n",
    "show_results(orig_text, gpc_accuracy, cross_val_gpc, conf_mat_gpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.7527173913043478\n",
      "Cross Validation Results:\t [0.7804878  0.80147059 0.78186275]\n",
      "Cross Validation Mean:\t\t 0.787940379403794\n",
      "Confusion Matrix:\n",
      " [[133  66]\n",
      " [ 25 144]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "gpc_smote = GaussianProcessClassifier()\n",
    "gpc_smote.fit(X_train, y_train)\n",
    "\n",
    "gpc_accuracy = gpc_smote.score(X_test, y_test)\n",
    "smote_dict['GPC'] = gpc_accuracy\n",
    "cross_val_gpc = cross_val_score(gpc_smote, X_smote, y_smote)\n",
    "conf_mat_gpc = confusion_matrix(y_test, gpc_smote.predict(X_test))\n",
    "\n",
    "show_results(orig_text, gpc_accuracy, cross_val_gpc, conf_mat_gpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without smote:\n",
      "MLP :   0.8443396226415094\n",
      "SVM :   0.9150943396226415\n",
      "KNN :   0.8820754716981132\n",
      "GPC :   0.9150943396226415\n",
      "\n",
      "\n",
      "with smote:\n",
      "MLP :   0.8668478260869565\n",
      "SVM :   0.7119565217391305\n",
      "KNN :   0.8016304347826086\n",
      "GPC :   0.7527173913043478\n"
     ]
    }
   ],
   "source": [
    "print('without smote:')\n",
    "for key, val in orig_dict.items():\n",
    "    print(key, ':  ', val)\n",
    "\n",
    "print('\\n\\nwith smote:')\n",
    "for key, val in smote_dict.items():\n",
    "    print(key, ':  ', val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
