{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting some baselines from various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display metrics using show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_results(test_type, accuracy, cross_val, conf_mat):\n",
    "    print()\n",
    "    print(test_type)\n",
    "    print('Accuracy:\\t\\t\\t', accuracy)\n",
    "    print('Cross Validation Results:\\t', cross_val)\n",
    "    print('Cross Validation Mean:\\t\\t', np.mean(cross_val))\n",
    "    print('Confusion Matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            f   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            m   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            m   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            f   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            f   \n",
       "\n",
       "        ethnicity jundice autism        country app result  age_category  \\\n",
       "0  White-European      no     no  United States  no      6   18 and more   \n",
       "1          Latino      no    yes         Brazil  no      5   18 and more   \n",
       "2          Latino     yes    yes          Spain  no      8   18 and more   \n",
       "3  White-European      no    yes  United States  no      6   18 and more   \n",
       "4               ?      no     no          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Autism_Data.csv')\n",
    "df.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', \n",
    "              'A7', 'A8', 'A9', 'A10', 'age', 'gender',\n",
    "             'ethnicity', 'jundice', 'autism', 'country', \n",
    "             'app', 'result', 'age_category', 'relation',\n",
    "             'app_prediction']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert yes/no colums to 1/0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country</th>\n",
       "      <th>app</th>\n",
       "      <th>result</th>\n",
       "      <th>age_category</th>\n",
       "      <th>relation</th>\n",
       "      <th>app_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Latino</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>White-European</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>18 and more</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10      ...       gender  \\\n",
       "0   1   1   1   1   0   0   1   1   0    0      ...            0   \n",
       "1   1   1   0   1   0   0   0   1   0    1      ...            1   \n",
       "2   1   1   0   1   1   0   1   1   1    1      ...            1   \n",
       "3   1   1   0   1   0   0   1   1   0    1      ...            0   \n",
       "4   1   0   0   0   0   0   0   1   0    0      ...            0   \n",
       "\n",
       "        ethnicity jundice  autism        country app result  age_category  \\\n",
       "0  White-European       0       0  United States  no      6   18 and more   \n",
       "1          Latino       0       1         Brazil  no      5   18 and more   \n",
       "2          Latino       1       1          Spain  no      8   18 and more   \n",
       "3  White-European       0       1  United States  no      6   18 and more   \n",
       "4               ?       0       0          Egypt  no      2   18 and more   \n",
       "\n",
       "  relation app_prediction  \n",
       "0     Self             NO  \n",
       "1     Self             NO  \n",
       "2   Parent            YES  \n",
       "3     Self             NO  \n",
       "4        ?             NO  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_row = pd.Series(np.where(df['gender'] == 'm', 1, 0))\n",
    "jund_row = pd.Series(np.where(df['jundice'] == 'yes', 1, 0))\n",
    "autism_row = pd.Series(np.where(df['autism'] == 'yes', 1, 0))\n",
    "\n",
    "df['gender'] = gender_row\n",
    "df['jundice'] = jund_row\n",
    "df['autism'] = autism_row\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish X, y datasets.  \n",
    "### Set global paramaters for models found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'gender', 'jundice']]\n",
    "y = df['autism']\n",
    "\n",
    "test_size = 0.3\n",
    "random_state = 1\n",
    "alpha = 1e-5\n",
    "cv = 3\n",
    "\n",
    "# solver = {sgd, adam, lbfgs}\n",
    "solver = 'lbfgs'\n",
    "\n",
    "# hidden_layer_sizes = tuple, ith element is number of neurons in ith layer.\n",
    "# best results with {(30, 20, 10), (20, 30, 20, 10)}\n",
    "hidden_layer_sizes = (30, 20, 10)\n",
    "\n",
    "orig_text = 'Original data'\n",
    "smote_text = 'Preprocessed with SMOTE'\n",
    "\n",
    "# Dicts to keep track of scores\n",
    "orig_dict = {}\n",
    "smote_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.8490566037735849\n",
      "Cross Validation Results:\t [0.77118644 0.79059829 0.79059829]\n",
      "Cross Validation Mean:\t\t 0.7841276739581824\n",
      "Confusion Matrix:\n",
      " [[175  19]\n",
      " [ 13   5]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "clf = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "orig_dict['MLP'] = accuracy\n",
    "cross_val = cross_val_score(clf, X, y, cv=cv)\n",
    "conf_mat = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "show_results(orig_text, accuracy, cross_val, conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier (data after SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.8490566037735849\n",
      "Cross Validation Results:\t [0.81707317 0.86764706 0.87009804]\n",
      "Cross Validation Mean:\t\t 0.8516060895903076\n",
      "Confusion Matrix:\n",
      " [[159  40]\n",
      " [ 17 152]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "clf = MLPClassifier(solver=solver, \n",
    "                    alpha=alpha, \n",
    "                    hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "smote_accuracy = clf.score(X_test, y_test)\n",
    "smote_dict['MLP'] = smote_accuracy\n",
    "cross_val = cross_val_score(clf, X_smote, y_smote, cv=cv)\n",
    "conf_mat = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "show_results(smote_text, accuracy, cross_val, conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.9150943396226415\n",
      "Cross Validation Results:\t [0.86864407 0.87179487 0.87179487]\n",
      "Cross Validation Mean:\t\t 0.8707446037954513\n",
      "Confusion Matrix:\n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "svm_accuracy = clf_svm.score(X_test, y_test)\n",
    "orig_dict['SVM'] = svm_accuracy\n",
    "cross_val_svm = cross_val_score(clf_svm, X, y, cv=cv)\n",
    "conf_mat_svm = confusion_matrix(y_test, clf_svm.predict(X_test))\n",
    "\n",
    "show_results(orig_text, svm_accuracy, cross_val_svm, conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.7336956521739131\n",
      "Cross Validation Results:\t [0.76097561 0.78186275 0.72058824]\n",
      "Cross Validation Mean:\t\t 0.7544755300494183\n",
      "Confusion Matrix:\n",
      " [[124  75]\n",
      " [ 23 146]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "clf_svm_smote = svm.SVC()\n",
    "clf_svm_smote.fit(X_train, y_train)\n",
    "\n",
    "svm_accuracy = clf_svm_smote.score(X_test, y_test)\n",
    "smote_dict['SVM'] = svm_accuracy\n",
    "cross_val_svm = cross_val_score(clf_svm_smote, X_smote, y_smote, cv=cv)\n",
    "conf_mat_svm = confusion_matrix(y_test, clf_svm_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, svm_accuracy, cross_val_svm, conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.8820754716981132\n",
      "Cross Validation Results:\t [0.83474576 0.85042735 0.85042735]\n",
      "Cross Validation Mean:\t\t 0.8452001545221884\n",
      "Confusion Matrix:\n",
      " [[186   8]\n",
      " [ 17   1]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_accuracy = knn.score(X_test, y_test)\n",
    "orig_dict['KNN'] = knn_accuracy\n",
    "cross_val_knn = cross_val_score(knn, X, y, cv=cv)\n",
    "conf_mat_knn = confusion_matrix(y_test, knn.predict(X_test))\n",
    "\n",
    "show_results(orig_text, knn_accuracy, cross_val_knn, conf_mat_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.7880434782608695\n",
      "Cross Validation Results:\t [0.79512195 0.80392157 0.78921569]\n",
      "Cross Validation Mean:\t\t 0.796086402040491\n",
      "Confusion Matrix:\n",
      " [[137  62]\n",
      " [ 16 153]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_smote.fit(X_train, y_train)\n",
    "\n",
    "knn_accuracy = knn_smote.score(X_test, y_test)\n",
    "smote_dict['KNN'] = knn_accuracy\n",
    "cross_val_knn = cross_val_score(knn_smote, X_smote, y_smote, cv=cv)\n",
    "conf_mat_knn = confusion_matrix(y_test, knn_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, knn_accuracy, cross_val_knn, conf_mat_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.9150943396226415\n",
      "Cross Validation Results:\t [0.86864407 0.87179487 0.87179487]\n",
      "Cross Validation Mean:\t\t 0.8707446037954513\n",
      "Confusion Matrix:\n",
      " [[194   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "gpc = GaussianProcessClassifier()\n",
    "gpc.fit(X_train, y_train)\n",
    "\n",
    "gpc_accuracy = gpc.score(X_test, y_test)\n",
    "orig_dict['GPC'] = gpc_accuracy\n",
    "cross_val_gpc = cross_val_score(gpc, X, y, cv=cv)\n",
    "conf_mat_gpc = confusion_matrix(y_test, gpc.predict(X_test))\n",
    "\n",
    "show_results(orig_text, gpc_accuracy, cross_val_gpc, conf_mat_gpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Process Classifier (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.7853260869565217\n",
      "Cross Validation Results:\t [0.77317073 0.80147059 0.80147059]\n",
      "Cross Validation Mean:\t\t 0.7920373027259684\n",
      "Confusion Matrix:\n",
      " [[137  62]\n",
      " [ 17 152]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "gpc_smote = GaussianProcessClassifier()\n",
    "gpc_smote.fit(X_train, y_train)\n",
    "\n",
    "gpc_accuracy = gpc_smote.score(X_test, y_test)\n",
    "smote_dict['GPC'] = gpc_accuracy\n",
    "cross_val_gpc = cross_val_score(gpc_smote, X_smote, y_smote, cv=cv)\n",
    "conf_mat_gpc = confusion_matrix(y_test, gpc_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, gpc_accuracy, cross_val_gpc, conf_mat_gpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data\n",
      "Accuracy:\t\t\t 0.9150943396226415\n",
      "Cross Validation Results:\t [0.86864407 0.85470085 0.88034188]\n",
      "Cross Validation Mean:\t\t 0.8678956009464484\n",
      "Confusion Matrix:\n",
      " [[193   1]\n",
      " [ 17   1]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "\n",
    "lgr_acc = lgr.score(X_test, y_test)\n",
    "orig_dict['lgr'] = lgr_acc\n",
    "cross_val_lgr = cross_val_score(lgr, X, y, cv=cv)\n",
    "conf_mat_lgr = confusion_matrix(y_test, lgr.predict(X_test))\n",
    "\n",
    "show_results(orig_text, lgr_acc, cross_val_lgr, conf_mat_lgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression after SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed with SMOTE\n",
      "Accuracy:\t\t\t 0.6766304347826086\n",
      "Cross Validation Results:\t [0.6902439  0.67647059 0.66176471]\n",
      "Cross Validation Mean:\t\t 0.6761597321855571\n",
      "Confusion Matrix:\n",
      " [[123  76]\n",
      " [ 43 126]]\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "lgr_smote = LogisticRegression()\n",
    "lgr_smote.fit(X_smote, y_smote)\n",
    "\n",
    "lgr_acc_smote = lgr_smote.score(X_test, y_test)\n",
    "smote_dict['lgr'] = lgr_acc_smote\n",
    "cross_val_lgr = cross_val_score(lgr_smote, X_smote, y_smote, cv=cv)\n",
    "conf_mat_lgr = confusion_matrix(y_test, lgr_smote.predict(X_test))\n",
    "\n",
    "show_results(smote_text, lgr_acc_smote, cross_val_lgr, conf_mat_lgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without smote:\n",
      "MLP :   0.8490566037735849\n",
      "SVM :   0.9150943396226415\n",
      "KNN :   0.8820754716981132\n",
      "GPC :   0.9150943396226415\n",
      "lgr :   0.9150943396226415\n",
      "\n",
      "\n",
      "with smote:\n",
      "MLP :   0.845108695652174\n",
      "SVM :   0.7336956521739131\n",
      "KNN :   0.7880434782608695\n",
      "GPC :   0.7853260869565217\n",
      "lgr :   0.6766304347826086\n"
     ]
    }
   ],
   "source": [
    "print('without smote:')\n",
    "for key, val in orig_dict.items():\n",
    "    print(key, ':  ', val)\n",
    "\n",
    "print('\\n\\nwith smote:')\n",
    "for key, val in smote_dict.items():\n",
    "    print(key, ':  ', val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
